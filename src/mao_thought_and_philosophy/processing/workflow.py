import re
from pathlib import Path

# ä¿æŒåŸæœ‰çš„å¯¼å…¥ä¸å˜
from .prompt_templates import get_user_prompt, get_system_prompt
from ..config import ASSETS_DIR, OUTPUT_DIR
from ..core.graph_builder import ConceptMemory
from ..core.llm_client import call_llm_json
from ..core.loader import read_epub_chapters_custom

# å®šä¹‰è¾“å‡ºè·¯å¾„
KB_DIR = OUTPUT_DIR / "knowledge_base"
CHAPTERS_DIR = KB_DIR / "chapters"
CONCEPTS_DIR = KB_DIR / "concepts"  # ã€æ–°å¢ã€‘æ¦‚å¿µå¡ç‰‡ç›®å½•


def sanitize_filename(name):
    """æ¸…æ´—æ–‡ä»¶å"""
    name = name.replace('.html', '').replace('.xhtml', '')
    name = re.sub(r'[\\/*?:"<>|â€œâ€â€˜â€™\'"]', "", name)
    return name.strip()[:60]


def get_safe_title_from_chap(chapter_data):
    """è¾…åŠ©å‡½æ•°ï¼šä»ç« èŠ‚æ•°æ®ä¸­æå–å¹¶æ¸…æ´—æ ‡é¢˜"""
    raw = chapter_data.get('title', chapter_data['id'])
    return sanitize_filename(raw)


def generate_concept_cards(memory):
    """
    å°† JSON æ•°æ®è½¬åŒ–ä¸º Obsidian å¯è¯»çš„ Markdown æ¦‚å¿µå¡ç‰‡
    """
    CONCEPTS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ“‡ æ­£åœ¨ç”Ÿæˆ {len(memory.concepts)} å¼ æ¦‚å¿µå¡ç‰‡...")

    for name, definition in memory.concepts.items():
        # æ¸…æ´—æ–‡ä»¶å
        safe_name = sanitize_filename(name)
        if not safe_name: continue

        file_path = CONCEPTS_DIR / f"{safe_name}.md"

        # è·å–å‡ºå¤„åˆ—è¡¨ (è¿™é‡Œå­˜å‚¨çš„æ˜¯æ–‡ä»¶åï¼Œå¦‚ "01_çœå§”...")
        chapter_links = memory.appearances.get(name, [])
        # ç”ŸæˆåŒå‘é“¾æ¥å­—ç¬¦ä¸²
        backlinks = ", ".join([f"[[{link}]]" for link in chapter_links])

        content = f"""---
tags: [æ ¸å¿ƒæ¦‚å¿µ]
---

# {name}

### ğŸ“ å®šä¹‰
> {definition}

### ğŸ“š å‡ºç°ç« èŠ‚
{backlinks}
"""
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)

    print(f"âœ… æ¦‚å¿µå¡ç‰‡å·²ç”Ÿæˆè‡³: {CONCEPTS_DIR}")


def run_analysis():
    # 1. åˆå§‹åŒ–ç›®å½•
    KB_DIR.mkdir(parents=True, exist_ok=True)
    CHAPTERS_DIR.mkdir(parents=True, exist_ok=True)

    # 2. åŠ è½½ç”µå­ä¹¦
    epub_path = ASSETS_DIR / "æ¯›ä¸»å¸­æ•™æˆ‘ä»¬å½“çœå§”ä¹¦è®°.epub"
    if not epub_path.exists():
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {ASSETS_DIR} ä¸‹æ‰¾ä¸åˆ°ç”µå­ä¹¦æ–‡ä»¶ï¼")
        return

    book_title = epub_path.stem
    print(f"ğŸ“– æ­£åœ¨è§£æã€Š{book_title}ã€‹...")

    current_system_prompt = get_system_prompt(book_title)
    chapters = read_epub_chapters_custom(epub_path)

    # ã€ä¿®æ”¹ç‚¹ 1ã€‘åˆå§‹åŒ–å¹¶åŠ è½½æ—§è®°å¿†
    # è¿™æ ·å³ä½¿è·³è¿‡å‰9ç« ï¼Œå‰9ç« çš„æ¦‚å¿µä¾ç„¶åœ¨å†…å­˜é‡Œï¼Œä¸ä¼šä¸¢å¤±
    memory = ConceptMemory()
    json_path = KB_DIR / "knowledge_graph.json"
    memory.load_from_file(json_path)

    print(f"ğŸ“š å…±è¯†åˆ«å‡º {len(chapters)} ä¸ªç« èŠ‚ï¼Œå¼€å§‹æ„å»ºçŸ¥è¯†åº“...\n")

    index_content = "# å…¨ä¹¦ç›®å½•ä¸ç´¢å¼•\n\n| åºå· | ç« èŠ‚ | æ ¸å¿ƒæ ‡ç­¾ | ä¸€å¥è¯æ€»ç»“ |\n|---|---|---|---|\n"

    # 3. é€ç« å¤„ç†
    for i, chap in enumerate(chapters):

        # --- A. å‡†å¤‡æ–‡ä»¶å ---
        curr_safe_title = get_safe_title_from_chap(chap)
        file_name = f"{i + 1:02d}_{curr_safe_title}.md"
        file_path = CHAPTERS_DIR / file_name

        # è¿™æ˜¯ä¸€ä¸ªä¸å¸¦åç¼€çš„é“¾æ¥åï¼Œç”¨äº Obsidian é“¾æ¥å’Œè®°å¿†åº“
        link_name = f"{i + 1:02d}_{curr_safe_title}"

        # =================================================================
        # æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        # =================================================================
        if file_path.exists():
            print(f"â© [å·²å­˜åœ¨ï¼Œè·³è¿‡] {file_name}")

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    # ç®€å•æå–æ‘˜è¦å’Œæ ‡ç­¾ç”¨äºç›®å½•å›å¡«
                    summary_match = re.search(r'> \*\*æ‘˜è¦\*\*ï¼š(.*?)\n', content)
                    summary = summary_match.group(1).strip() if summary_match else "ï¼ˆæ‘˜è¦è¯»å–å¤±è´¥ï¼‰"

                    tags_match = re.search(r'tags: \[(.*?)\]', content)
                    tags_str = tags_match.group(1) if tags_match else ""
                    tags_clean = tags_str.replace("'", "").replace('"', "")
                    tags_display = ", ".join([f"`{t.strip()}`" for t in tags_clean.split(',')][:3])

                    link = f"[{curr_safe_title}](./chapters/{file_name})"
                    index_content += f"| {i + 1} | {link} | {tags_display} | {summary} |\n"
            except Exception as e:
                print(f"   âš ï¸ è¯»å–æ—§æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")

            continue
        # =================================================================

        print(f"âš¡ [{i + 1}/{len(chapters)}] æ­£åœ¨æ·±åº¦ç ”è¯»ï¼š{curr_safe_title} ...")

        # --- B. AI åˆ†æ ---
        context_str = memory.get_context_string()
        prompt = get_user_prompt(chap['content'], context_str)

        try:
            result = call_llm_json(current_system_prompt, prompt)
        except Exception as e:
            print(f"   âš ï¸ åˆ†æå¤±è´¥ï¼Œè·³è¿‡æœ¬ç« : {str(e)}")
            continue

        # --- C. æ›´æ–°çŸ¥è¯†å›¾è°±è®°å¿† ---
        # ã€ä¿®æ”¹ç‚¹ 2ã€‘ä¼ å…¥ link_name (å¸¦åºå·çš„)ï¼Œè¿™æ ·æ¦‚å¿µå¡ç‰‡é‡Œçš„é“¾æ¥å°±èƒ½ç‚¹é€šäº†
        concepts = result.get('key_concepts', [])
        memory.update(concepts, link_name)

        # --- D. ç»„è£… Markdown å†…å®¹ ---
        tags = result.get('tags', [])
        summary = result.get('summary', 'æš‚æ— æ€»ç»“').replace('"', "'")

        md_content = f"""---
title: "{curr_safe_title}"
order: {i + 1}
tags: {tags}
date: 2025-11-30
---

# ç¬¬{i + 1}ç«  {curr_safe_title}

> **æ‘˜è¦**ï¼š{summary}

"""
        # ä¿®å¤åŸæ–‡åˆ†æ®µæ˜¾ç¤ºé—®é¢˜
        html_formatted_content = chap['content'].replace('\n', '<br>')
        md_content += f"""
<details>
<summary><strong>ğŸ“„ ç‚¹å‡»å±•å¼€/æ”¶èµ·ï¼šæœ¬ç« åŸæ–‡å…¨æ–‡</strong></summary>

{html_formatted_content}

</details>

"""
        md_content += f"""
## ğŸ§  æ·±åº¦æ€è€ƒä¸è§£è¯»

{result.get('analysis')}

"""

        if 'quotes' in result and result['quotes']:
            md_content += "### ğŸ’¬ æŒ¯è‹å‘è©çš„é‡‘å¥\n"
            for q in result['quotes']:
                md_content += f"> {q}\n>\n"

        md_content += "\n---\n"

        # ä¸Šä¸€ç« é“¾æ¥
        if i > 0:
            prev_chap = chapters[i - 1]
            prev_title = get_safe_title_from_chap(prev_chap)
            prev_link_name = f"{i:02d}_{prev_title}"
            md_content += f"â¬…ï¸ ä¸Šä¸€ç« ï¼š[[{prev_link_name}]] | "

        # ä¸‹ä¸€ç« é“¾æ¥
        if i < len(chapters) - 1:
            next_chap = chapters[i + 1]
            next_title = get_safe_title_from_chap(next_chap)
            next_link_name = f"{i + 2:02d}_{next_title}"
            md_content += f"ä¸‹ä¸€ç« ï¼š[[{next_link_name}]] â¡ï¸"

        # --- E. å†™å…¥æ–‡ä»¶ ---
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(md_content)

        # --- F. æ›´æ–°ç›®å½•ç´¢å¼• ---
        tags_str = ", ".join([f"`{t}`" for t in tags[:3]])
        link = f"[{curr_safe_title}](./chapters/{file_name})"
        index_content += f"| {i + 1} | {link} | {tags_str} | {summary} |\n"

        # ã€å»ºè®®ã€‘æ¯è·‘å®Œä¸€ç« å°±ä¿å­˜ä¸€æ¬¡è®°å¿†ï¼Œé˜²æ­¢ç¨‹åºä¸­é€”å´©æºƒæ•°æ®ä¸¢å¤±
        memory.save_memory(KB_DIR)

    # 4. å¾ªç¯ç»“æŸåçš„æ”¶å°¾å·¥ä½œ
    with open(KB_DIR / "00_å…¨ä¹¦æ¦‚è§ˆ_Index.md", "w", encoding="utf-8") as f:
        f.write(index_content)

    # ä¿å­˜æœ€ç»ˆ JSON
    memory.save_memory(KB_DIR)

    # ã€ä¿®æ”¹ç‚¹ 3ã€‘ç”Ÿæˆæ¦‚å¿µå¡ç‰‡
    generate_concept_cards(memory)

    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼çŸ¥è¯†åº“å·²ç”Ÿæˆåœ¨ï¼š{KB_DIR}")
    print("ä½ å¯ä»¥ç›´æ¥ç”¨ Obsidian æ‰“å¼€æ­¤ç›®å½•ï¼Œä½“éªŒæœ€ä½³ã€‚")
